{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividades Prácticas\n",
    "1. Configurar entorno de desarrollo python.\n",
    "2. Instalar dependencias 'pip install openai transformers torch'.\n",
    "3. Hola mundo ya sea OpenAI o HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # Importa la biblioteca de OpenAI para interactuar con su API.\n",
    "openai.api_key = \"TU_API_KEY\"  # Configura tu clave API para autenticarte con OpenAI.\n",
    "respuesta = openai.Completion.create(  # Llama a la API de OpenAI para generar texto.\n",
    "    engine=\"text-davinci-003\", # Especifica el modelo a utilizar (por ejemplo, text-davinci-003).\n",
    "    prompt=\"Hola, ¿cómo estás?\",  # Define el texto inicial o pregunta para el modelo.\n",
    "    max_tokens=50  # Limita la cantidad máxima de tokens en la respuesta generada.\n",
    ")\n",
    "print(respuesta.choices[0].text.strip())  # Imprime la respuesta generada, eliminando espacios innecesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, ¿How are you? ¿Do you feel good?\"\n",
      "\n",
      "\"I'm fine. I'm just really tired.\"\n",
      "\n",
      "\"What do you mean?\"\n",
      "\n",
      "\"I'm just going to sleep.\"\n",
      "\n",
      "\"What\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline  # Importa la función pipeline de la biblioteca Transformers.\n",
    "generador = pipeline(\"text-generation\", model=\"gpt2\")  # Configura un generador de texto usando el modelo GPT-2.\n",
    "texto = generador(\"Hi, ¿How are you?\",  # Genera texto basado en el prompt inicial.\n",
    "    max_length=50,  # Define la longitud máxima del texto generado.\n",
    "    do_sample=True,  # Activa el muestreo para generar texto más variado.\n",
    "    temperature=0.5,  # Controla la aleatoriedad; valores bajos producen texto más coherente.\n",
    "    top_k=50,  # Considera las 50 palabras más probables en cada paso de generación. (OPCIONAL)\n",
    "    top_p=0.95  # Usa el enfoque de muestreo de núcleo para limitar la probabilidad acumulada. (OPCIONAL)\n",
    "    )\n",
    "print(texto[0]['generated_text'])  # Imprime el texto generado por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, ¿How are you? How are you?\n",
      "\n",
      "I am fine and you are fine.\n",
      "\n",
      "I am fine, how are you?\n",
      "\n",
      "I am fine, how are you?\n",
      "\n",
      "I am fine and you are fine.\n",
      "\n",
      "- ¿Qué ves?\n",
      "\n",
      "- Mucho bonito.\n",
      "\n",
      "- ¿Qué?\n",
      "\n",
      "- You are beautiful.\n",
      "\n",
      "- ¿Cómo lo sé?\n",
      "\n",
      "- You are beautiful.\n",
      "\n",
      "- ¡Qué, si se pone más bonito!\n",
      "\n",
      "- I am beautiful.\n",
      "\n",
      "- ¿Qué?\n",
      "\n",
      "- You are beautiful.\n",
      "\n",
      "- ¡Oh, pero no me gusta esto!\n",
      "\n",
      "- I am beautiful.\n",
      "\n",
      "- ¿Qué?\n",
      "\n",
      "- You are beautiful.\n",
      "\n",
      "- ¡Y está bien, ¿no?\n",
      "\n",
      "- You\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline  # Importa la función pipeline de la biblioteca Transformers.\n",
    "generador = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "texto_coherente = generador(\n",
    "    \"Hi, ¿How are you?\",\n",
    "    max_length=200,  # Longitud máxima del texto generado\n",
    "    do_sample=True,  # Activa el muestreo\n",
    "    temperature=0.7,  # Reduce la aleatoriedad para mayor coherencia\n",
    "    # top_k=10,  # Considera solo las 10 palabras más probables\n",
    "    # top_p=0.9  # Limita la probabilidad acumulada\n",
    ")\n",
    "print(texto_coherente[0]['generated_text'])  # Imprime el texto generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, ¿How are you? I’m still in the middle of a project and I’m just now getting back to you.\n",
      "\n",
      "I’m not sure what you mean by “I’m not\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generador = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "texto_coherente = generador(\n",
    "    \"Hi, ¿How are you?\",\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "print(texto_coherente[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me two examples of linear equations, without text, and with text.\n",
      "\n",
      "A:\n",
      "\n",
      "I think this is a good example of a linear equation. \n",
      "$$\\begin{align}\n",
      "x^2 + y^2 - z^2 &= 1 \\\\\n",
      "(x-1)^2 + (y-1)^2 - (z-1)^2 &= 1 \\\\\n",
      "(x-1)(y-1)(z-1) &= 1\n",
      "\\end{align}$$\n",
      "This is a quadratic equation in $x,y,z$ with a single real solution $x=1,y=1,z=1$.\n",
      "\n",
      "A:\n",
      "\n",
      "This is a quadratic equation in $x,y,z$ with a single real solution $x=1,y=1,z=1$.\n",
      "\n",
      "This is not a linear equation. It's a quadratic equation in $\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generador = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "texto_coherente = generador(\n",
    "    \"Give me two examples of linear equations, without text\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "print(texto_coherente[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
