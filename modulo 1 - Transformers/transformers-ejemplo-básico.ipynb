{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers de Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instalaci贸n dependencias \n",
    "    ```\n",
    "    pip install transformers torch\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. C贸digo b谩sico para generaci贸n de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Texto generado:\n",
      "\n",
      "Una vez en un bosque encantado, un joven aventurero encontr贸 en conicuente, una vista en efectador, mi que esta una segunda, una un\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Creamos un pipeline de generaci贸n de texto con GPT-2\n",
    "generador = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Prompt de entrada (puedes cambiarlo)\n",
    "prompt = \"Una vez en un bosque encantado, un joven aventurero encontr贸\"\n",
    "\n",
    "# Generaci贸n de texto\n",
    "respuestas = generador(prompt, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\" Texto generado:\\n\")\n",
    "print(respuestas[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Creamos un pipeline de generaci贸n de texto con GPT-2\n",
    "generador = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Texto generado:\n",
      "\n",
      "Una vez en un bosque encantado, un joven aventurero encontr贸 una p贸lvora en el bosque. La herida lo llev贸 a la morada de los campos. La\n"
     ]
    }
   ],
   "source": [
    "# Prompt de entrada (puedes cambiarlo)\n",
    "prompt = \"Una vez en un bosque encantado, un joven aventurero encontr贸\"\n",
    "\n",
    "# Generaci贸n de texto\n",
    "respuestas = generador(prompt, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\" Texto generado:\\n\")\n",
    "print(respuestas[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Texto generado:\n",
      "\n",
      "驴Cu谩les son los colores primarios?\n",
      "\n",
      "El color primario es el color que se pone en el fondo de la piel, y es el color que se pone en el fondo de la piel de los animales.\n",
      "\n",
      "El color primario es el color que se pone en el fondo de la piel de los animales.\n",
      "\n",
      "El color primario es el color que se pone en el fondo de la\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Prompt de entrada (puedes cambiarlo)\n",
    "prompt = \"驴Cu谩les son los colores primarios?\"\n",
    "\n",
    "# Generaci贸n de texto\n",
    "respuestas = generador(prompt, max_length=100, temperature=0.2)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\" Texto generado:\\n\")\n",
    "print(respuestas[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
