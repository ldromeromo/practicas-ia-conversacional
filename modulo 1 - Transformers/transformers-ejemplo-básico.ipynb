{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers de Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instalación dependencias \n",
    "    ```\n",
    "    pip install transformers torch\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Código básico para generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Texto generado:\n",
      "\n",
      "Una vez en un bosque encantado, un joven aventurero encontró en conicuente, una vista en efectador, mi que esta una segunda, una un\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Creamos un pipeline de generación de texto con GPT-2\n",
    "generador = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Prompt de entrada (puedes cambiarlo)\n",
    "prompt = \"Una vez en un bosque encantado, un joven aventurero encontró\"\n",
    "\n",
    "# Generación de texto\n",
    "respuestas = generador(prompt, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\"🔮 Texto generado:\\n\")\n",
    "print(respuestas[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Creamos un pipeline de generación de texto con GPT-2\n",
    "generador = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Texto generado:\n",
      "\n",
      "Una vez en un bosque encantado, un joven aventurero encontró una pólvora en el bosque. La herida lo llevó a la morada de los campos. La\n"
     ]
    }
   ],
   "source": [
    "# Prompt de entrada (puedes cambiarlo)\n",
    "prompt = \"Una vez en un bosque encantado, un joven aventurero encontró\"\n",
    "\n",
    "# Generación de texto\n",
    "respuestas = generador(prompt, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\"🔮 Texto generado:\\n\")\n",
    "print(respuestas[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Texto generado:\n",
      "\n",
      "¿Cuáles son los colores primarios?\n",
      "\n",
      "El color primario es el color que se pone en el fondo de la piel, y es el color que se pone en el fondo de la piel de los animales.\n",
      "\n",
      "El color primario es el color que se pone en el fondo de la piel de los animales.\n",
      "\n",
      "El color primario es el color que se pone en el fondo de la\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Prompt de entrada (puedes cambiarlo)\n",
    "prompt = \"¿Cuáles son los colores primarios?\"\n",
    "\n",
    "# Generación de texto\n",
    "respuestas = generador(prompt, max_length=100, temperature=0.2)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\"🔮 Texto generado:\\n\")\n",
    "print(respuestas[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
