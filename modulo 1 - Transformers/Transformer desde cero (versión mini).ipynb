{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura del Transformer Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîß 1. Embedding + Positional Encodin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importa PyTorch, una biblioteca para computaci√≥n num√©rica y aprendizaje profundo\n",
    "import torch.nn as nn  # Importa el m√≥dulo de redes neuronales de PyTorch\n",
    "import math  # Importa la biblioteca matem√°tica est√°ndar de Python\n",
    "\n",
    "class PositionalEncoding(nn.Module):  # Define una clase para codificaci√≥n posicional, hereda de nn.Module\n",
    "    def __init__(self, d_model, max_len=5000):  # Constructor, inicializa los par√°metros del modelo\n",
    "        super().__init__()  # Llama al constructor de la clase base\n",
    "        pe = torch.zeros(max_len, d_model)  # Crea un tensor de ceros para almacenar las codificaciones posicionales\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Genera √≠ndices de posici√≥n y a√±ade una dimensi√≥n\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))  # Calcula el t√©rmino de divisi√≥n para las frecuencias\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Aplica la funci√≥n seno a las posiciones pares\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Aplica la funci√≥n coseno a las posiciones impares\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)  # A√±ade una dimensi√≥n para el batch\n",
    "\n",
    "    def forward(self, x):  # M√©todo forward, aplica la codificaci√≥n posicional a la entrada\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)  # Suma las codificaciones posicionales a la entrada\n",
    "        return x  # Devuelve la entrada modificada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö° 2. Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):  # Define una clase para la atenci√≥n propia, hereda de nn.Module\n",
    "    def __init__(self, embed_dim):  # Constructor, inicializa los par√°metros del modelo\n",
    "        super().__init__()  # Llama al constructor de la clase base\n",
    "        self.embed_dim = embed_dim  # Dimensi√≥n del embedding\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)  # Proyecci√≥n lineal para queries, keys y values\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)  # Proyecci√≥n lineal para la salida\n",
    "\n",
    "    def forward(self, x):  # M√©todo forward, aplica la atenci√≥n propia\n",
    "        B, T, C = x.shape  # Obtiene las dimensiones del batch, secuencia y embedding\n",
    "        qkv = self.qkv(x)  # Calcula queries, keys y values concatenados\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Divide qkv en tres partes: queries, keys y values\n",
    "\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(C)  # Calcula las puntuaciones de atenci√≥n\n",
    "        weights = scores.softmax(dim=-1)  # Aplica softmax para obtener los pesos de atenci√≥n\n",
    "        attended = weights @ v  # Calcula la salida atendida\n",
    "\n",
    "        return self.out(attended)  # Devuelve la salida proyectada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé© 3. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):  # Define una clase para la atenci√≥n multi-cabeza, hereda de nn.Module\n",
    "    def __init__(self, embed_dim, num_heads):  # Constructor, inicializa los par√°metros del modelo\n",
    "        super().__init__()  # Llama al constructor de la clase base\n",
    "        assert embed_dim % num_heads == 0  # Verifica que embed_dim sea divisible por num_heads\n",
    "        self.head_dim = embed_dim // num_heads  # Calcula la dimensi√≥n de cada cabeza\n",
    "        self.num_heads = num_heads  # N√∫mero de cabezas\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)  # Proyecci√≥n lineal para queries, keys y values\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)  # Proyecci√≥n lineal para la salida\n",
    "\n",
    "    def forward(self, x):  # M√©todo forward, aplica la atenci√≥n multi-cabeza\n",
    "        B, T, C = x.shape  # Obtiene las dimensiones del batch, secuencia y embedding\n",
    "        qkv = self.qkv(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  # Calcula y reorganiza qkv\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Divide qkv en queries, keys y values\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # Calcula las puntuaciones de atenci√≥n\n",
    "        weights = scores.softmax(dim=-1)  # Aplica softmax para obtener los pesos de atenci√≥n\n",
    "        context = weights @ v  # Calcula el contexto atendido\n",
    "\n",
    "        out = context.transpose(1, 2).contiguous().view(B, T, C)  # Reorganiza y combina las cabezas\n",
    "        return self.out(out)  # Devuelve la salida proyectada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß± 4. Bloque Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):  # Define un bloque Transformer, hereda de nn.Module\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):  # Constructor, inicializa los par√°metros del modelo\n",
    "        super().__init__()  # Llama al constructor de la clase base\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)  # Crea una capa de atenci√≥n multi-cabeza\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)  # Normalizaci√≥n por capas para la salida de atenci√≥n\n",
    "        self.ff = nn.Sequential(  # Crea una red feed-forward\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),  # Proyecci√≥n lineal hacia una dimensi√≥n oculta\n",
    "            nn.ReLU(),  # Funci√≥n de activaci√≥n ReLU\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)  # Proyecci√≥n lineal de vuelta a la dimensi√≥n original\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)  # Normalizaci√≥n por capas para la salida de la red feed-forward\n",
    "\n",
    "    def forward(self, x):  # M√©todo forward, aplica el bloque Transformer\n",
    "        x = x + self.attn(self.norm1(x))  # Aplica atenci√≥n multi-cabeza con residual\n",
    "        x = x + self.ff(self.norm2(x))  # Aplica red feed-forward con residual\n",
    "        return x  # Devuelve la salida del bloque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† 5. Mini GPT (Tokenizador muy simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformerModel(nn.Module):  # Define un modelo Transformer mini, hereda de nn.Module\n",
    "    def __init__(self, vocab_size, embed_dim, max_len, num_heads, ff_hidden_dim, num_layers):  # Constructor\n",
    "        super().__init__()  # Llama al constructor de la clase base\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)  # Embedding para los tokens\n",
    "        self.pos_embed = PositionalEncoding(embed_dim, max_len)  # Codificaci√≥n posicional\n",
    "        self.transformer_blocks = nn.Sequential(*[  # Secuencia de bloques Transformer\n",
    "            TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
    "            for _ in range(num_layers)  # Repite el bloque num_layers veces\n",
    "        ])\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)  # Proyecci√≥n lineal para la salida del modelo\n",
    "\n",
    "    def forward(self, x):  # M√©todo forward, aplica el modelo completo\n",
    "        x = self.token_embed(x)  # Aplica el embedding de tokens\n",
    "        x = self.pos_embed(x)  # Aplica la codificaci√≥n posicional\n",
    "        x = self.transformer_blocks(x)  # Pasa por los bloques Transformer\n",
    "        return self.lm_head(x)  # Devuelve la salida proyectada (sin softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 100])\n"
     ]
    }
   ],
   "source": [
    "# Simulamos datos\n",
    "vocab_size = 100\n",
    "seq_len = 32\n",
    "batch_size = 8\n",
    "embed_dim = 64\n",
    "\n",
    "model = MiniTransformerModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    max_len=seq_len,\n",
    "    num_heads=4,\n",
    "    ff_hidden_dim=128,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "output = model(dummy_input)  # (batch_size, seq_len, vocab_size)\n",
    "print(output.shape)  # Debe ser (8, 32, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        chars = sorted(list(set(corpus)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(self.stoi)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[c] for c in text]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.itos[i] for i in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 4, 1]\n",
      "hola\n"
     ]
    }
   ],
   "source": [
    "text = \"hola mundo\"\n",
    "tokenizer = SimpleTokenizer(text)\n",
    "ids = tokenizer.encode(\"hola\")\n",
    "print(ids)  # ‚Üí lista de n√∫meros\n",
    "print(tokenizer.decode(ids))  # ‚Üí \"hola\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
